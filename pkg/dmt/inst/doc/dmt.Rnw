\documentclass[a4paper]{article}
\usepackage{amsmath}



\title{Dependency modelling toolbox}

\author{Leo Lahti\footnote{leo.lahti@iki.fi}, Olli-Pekka Huovilainen, Abhishek Tripathi,\\Ilkka Huopaniemi, and Tommi Suvitaival\\
  Department of Information and Computer Science,\\Aalto University
  School of Science and Technology, Finland}

\usepackage{Sweave}
\usepackage{float}
\begin{document}

\maketitle

%\VignetteIndexEntry{Dependency modelling toolbox}

\section{Introduction}

Investigation of dependencies between multiple data sources allows the
discovery of regularities and interactions that are not seen in
individual data sets. The importance of such methods is increasing
with the availability and size of co-occurring data sets in
computational biology, open data initiatives, and in other
domains. Practical, open access implementations of general-purpose
algorithms will help to realize the full potential of these
information sources.

This package provides general-purpose tools for the discovery and
analysis of statistical dependencies between co-occurring data
sources.  The implementations are based on well-established models
such as probabilistic canonical correlation analysis and multi-task
learning \cite{Archambeau06, Bach05, Huopaniemi10, Lahti09mlsp,
  Tripathi08}. Probabilistic framework deals rigorously with the
uncertainties associated with small sample sizes, and allows
incorporation of prior information in the analysis through Bayesian
priors \cite{Lahti09mlsp}. The applicability of the models has been
demonstrated in previous case studies \cite{Huopaniemi10, Lahti09mlsp,
  Tripathi08}.  This is a development version. Your feedback and
contributions are welcome. See the project page at
R-Forge\footnote{http://dmt.r-forge.r-project.org/}, or contact
project authors.



\section{Available functionality}

Current version is divided in three independent modules. We are
working on integrating these application-oriented modules into a
unified dependency modelling toolbox. The functionality includes

\begin{itemize}

\item regularized dependency detection (pint\footnote{http://bioconductor.org/packages/release/bioc/html/pint.html}; \cite{Lahti09mlsp})

\item dependency-based dimensionality reduction (drCCA\footnote{http://www.cis.hut.fi/projects/mi/software/drCCA/}; \cite{Tripathi08}) 

  
\item multi-way modeling of co-occurrence data (multiWayCCA\footnote{http://www.cis.hut.fi/projects/mi/software/multiWayCCA/}; \cite{Huopaniemi10})
\end{itemize}

Below is a brief summary of each module together with installation
instructions.

\section{Probabilistic dependency modeling framework (pint)}
\label{sec:framework}

\begin{figure}[htb]
\centering \includegraphics[height=3.5cm, keepaspectratio=TRUE]{cca2}
\caption{Graphical description of the shared latent variable model showing
generation of data sets $X$ and $Y$ from latent shared variable $\mathbf{z}$ 
through $W_x$ and $W_y$}
\label{modelpic}
\end{figure}

The pint
package\footnote{http://bioconductor.org/packages/release/bioc/html/pint.html};
\cite{Lahti09mlsp} implements the probabilistic dependency modeling
framework presented in \cite{Bach05} and its extensions
\cite{Archambeau06,Klami08,Lahti09mlsp}. This is a latent variable
model that assumes that the two data sets, \(X\) and \(Y\) can be
decomposed in {\it shared} and {\it data set-specific} components
(Figure~\ref{modelpic}). The model helps to discover the shared
components, given modeling assumptions.

The shared signal of two data sets is modeled with a shared latent
variable $\mathbf{z}$. Intuitively, this measures the strength of the
shared signal in each patient. While the variation is shared, it can
have different manifestation in each data set. This is described by
\(W_xz\) and \(W_yz\) where \(W_x\), \(W_y\) indicate how the shared
signal is observed in the individual data sets. Assuming a standard
Gaussian model for the shared latent variable \(\mathbf{z} \sim N(0, I)\)
and data set-specific effects, this leads to the following model:

\begin{equation}\label{eq:model}             
\begin{aligned}
  X \sim W_x\mathbf{z} + \varepsilon_x\\
Y \sim W_y\mathbf{z} + \varepsilon_y \\
\varepsilon_. \sim \mathcal{N}(0, \Psi_.)\\
\mathbf{z} \sim \mathcal{N}(0, I)   
\end{aligned}
\end{equation}

The data set-specific effects are described by the covariance matrices
$\Psi_x$, $\Psi_y$.  The model parameters are estimated with an
expectation-maximization (EM) algorithm.

\subsection{Special cases}

Special cases of the model include probabilistic versions of canonical
correlation analysis, factor analysis, and principal component
analysis, and regularized versions of them.

Probabilistic CCA (pCCA) assumes full covariance matrices \(\Psi_x\),
\(\Psi_y\). This gives the most detailed model for the data set
specific effects. The connection of this latent variable model and the
traditional canonical correlation analysis has been established in
\cite{Bach05}.

Probabilistic factor analysis (pFA) is obtained with a diagonal
covariances \(\Psi_x\), \(\Psi_y\). In addition, a special case is
implemented where each covariance matrix \(\Psi_.\) is isotropic but
they are not necessarily identical (as would be the case in
pPCA). This model is identical to concatenating \(X\), \(Y\), and
fitting ordinary probabilistic factor analysis on the concatenated
data set. The structure of the covariances is simpler than in
pCCA. This regularizes the solution and can potentially reduce
overfitting in some applications.

Probabilistic PCA (pPCA) is obtained with identical isotropic
covariances for the data set-specific effects: \(\Psi_x = \Psi_y =
\sigma I\). This model is identical to concatenating \(X\), \(Y\), and
fitting ordinary probabilistic PCA on the concatenated data set.

\subsection{Regularized dependency modeling}

We provide toos to guide dependency modeling through Bayesian priors
\cite{Lahti09mlsp}. Similarity-constrained probabilistic CCA (pSimCCA)
imposes a prior on the relation between $W_x$ and $W_y$. This can be
used to guide modeling to focus on certain types of dependencies, and
to avoid overfitting. The relationship is described through \(W_y =
TW_x\). A prior on \(T\) can be used to focus the modeling on certain
types of dependencies. We use matrix normal prior distribution: \(P(T)
= N_m(H, \sigma^2_TI, \sigma^2_TI)\). By default, $H = I$ and
\(\sigma^2_T = 0\), which results in identical manifestation of the
shared signal in the two data sets: $W_y = W_x$. This model is denoted
pSimCCA in the package. However, the prior can be loosened by tuning
$sigma^2_T$. With $sigma^2_T \rightarrow \infty$, estimation of $W_x$
and $W_y$ become independent, which leads to ordinary probabilistic
CCA. It is also possible to tune the mean matrix $H$. This would set a
particular relationship between the manifestations of the shared
component in each data set, and \(sigma^2_T\) is again be used to tune
the strength of such prior.


\subsection{Installing pint}

Install pint from within R with command\\
source('http://bioconductor.org/biocLite.R')\\
biocLite('pint')

\subsection{Documentation of pint}

The package implements the dependency modeling framework explained
above (see function 'fit.dependency.model'), and provides wrappers for
the special cases of the model. The documentation is available at\\
http://bioconductor.org/packages/2.6/bioc/vignettes/pint/inst/doc/depsearch.pdf

\subsection{Applications of pint}

pint has been used to integrate gene expression and copy number (aCGH)
data to discover cancer-associated chromosomal regions. See
\cite{Lahti09mlsp} for details.


\section{Dependency-based dimensionality reduction (drCCA)}

The drCCA\footnote{http://www.cis.hut.fi/projects/mi/software/drCCA/}
\cite{Tripathi08} method retains the variation that is shared between
the original data sources, while reducing the dimensionality by
ignoring variation that is specific to any of the data sources
alone. This captures the common variation that is shared by the
measurement sources. Note that the shared variation can be reflected
in different ways in the different data sets.  The approach utilizes
generalized canonical correlation analysis to perform a linear
projection on the collection data sets. Linearity makes it fast on
large data sets. The package includes regularization and tools for
selecting the final dimensionality of the combined data set
automatically.

\subsection{Installing drCCA}

Install drCCA from within R with command\\
'install.packages("drCCA", repos="http://R-Forge.R-project.org")'

\subsection{Documentation of drCCA}

Currently only online-documentation for the package is available. See\\
http://www.cis.hut.fi/projects/mi/software/drCCA/dochtml/00Index.html

\subsection{Applications of drCCA}

drCCA has been applied to dimensionality reduction in functional
genomics \cite{Tripathi08}. It combines information from several
measurement sources. This helps to reduce the noise in biological
experiments with high dimensionality but small sample size. The drCCA
can be used to summarize shared variation in several data sources with
co-occurring samples into a one vectorial data set of lower
dimensionality which captures the shared effects of the data sources.


\section{Multi-way multi-view models (multiWayCCA)}

multiWayCCA\footnote{http://www.cis.hut.fi/projects/mi/software/multiWayCCA/}
provides tools for multi-way, multi-source modeling. This is
particularly usefule for simultaneous multi-way (anova-type) modelling
of multiple related data sources. For details, see the original paper
\cite{Huopaniemi10}.

\subsection{Installing multiWayCCA}

multiWayCCA is currently available only as example source code. The
sources contain both code and examples. Download the
source\footnote{http://www.cis.hut.fi/projects/mi/software/multiWayCCA/multiWayCCA-package-100326.zip}. Then
uncompress the folder; readme.txt in the uncompressed folder contains
instructions for running the analysis. For application examples, see
\cite{Huopaniemi10}.

\subsection{Documentation of multiWayCCA}

For documentation and examples, see the readme.txt file included in
the package.

\subsection{Applications of multiWayCCA: metabolomics}

multiWayCCA has been applied in high-throughput metabolomics studies,
see \cite{Huopaniemi10} for details.

\subsection{Licensing terms}

Currently the different modules have different licensing terms. The
licenses will be unified in the integrated version.

\begin{itemize}
\item pint: GNU GPL \(>=2\)
\item drCCA: GNU LGPL \(>=2.1\)
\item multiWayCCA: no license associated with the code at the moment
\end{itemize}

\subsection*{Acknowledgements}

The project is a joint effort by several people: Leo Lahti, Tommi
Suvitaival, Olli-Pekka Huovilainen, Ilkka Huopaniemi, Abhishek
Tripathi, Arto Klami, and Samuel Kaski from the Statistical Machine
Learning and Bioinformatics group at the Department of Information and
Computer Science, Aalto University School of Science and Technology,
Finland. Abhishek Tripathi is with Department of Computer Science,
University of Helsinki, Finland. The authors belong also to Helsinki
Institute for Information Technology HIIT and Adaptive Informatics
Research Centre AIRC.


%\bibliographystyle{unsrt}
%\bibliography{dmt}

\begin{thebibliography}{1}

\bibitem{Archambeau06}
C{\'e}dric Archambeau, Nicolas Delannay, and Michel Verleysen.
\newblock Robust probabilistic projections.
\newblock In W.W. Cohen and A.~Moore, editors, {\em Proceedings of the 23rd
  International conference on machine learning}, pages 33--40. ACM, 2006.

\bibitem{Bach05}
Francis~R. Bach and Michael~I. Jordan.
\newblock A probabilistic interpretation of canonical correlation analysis.
\newblock Technical Report 688, Department of Statistics, University of
  California, Berkeley, 2005.

\bibitem{Huopaniemi10}
Ilkka Huopaniemi, Tommi Suvitaival, Janne Nikkil\"{a}, Matej Ore\u{s}ic, and
  Samuel Kaski.
\newblock Multivariate multi-way analysis of multi-source data.
\newblock {\em Bioinformatics}, 2010.
\newblock (ISMB 2010, to appear).

\bibitem{Lahti09mlsp}
Leo Lahti, Samuel Myllykangas, Sakari Knuutila, and Samuel Kaski.
\newblock Dependency detection with similarity constraints.
\newblock In {\em Proc. MLSP'09 IEEE International Workshop on Machine Learning
  for Signal Processing}, 2009.

\bibitem{Tripathi08}
Arto~Klami Abhishek~Tripathi and Samuel Kaski.
\newblock Simple integrative preprocessing preserves what is shared in data
  sources.
\newblock {\em BMC Bioinformatics}, 9(111), 2008.

\bibitem{Klami08}
Arto Klami and Samuel Kaski.
\newblock Probabilistic approach to detecting dependencies between data sets.
\newblock {\em Neurocomputing}, 72(1-3):39--46, 2008.

\end{thebibliography}


\end{document}
